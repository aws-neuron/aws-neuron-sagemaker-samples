{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1849fdfc-f4e9-422d-9561-1886491bccd9",
   "metadata": {},
   "source": [
    "# Compiling and Deploying HuggingFace Pretrained BERT on Inf2 on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc79137e-3548-4119-907b-f2a932664101",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4db48-e101-43eb-8b5c-9d8abebdfb9a",
   "metadata": {},
   "source": [
    "In this tutotial we will compile and deploy a pretrained BERT Base model from HuggingFace Transformers, using the [AWS Deep Learning Containers](https://github.com/aws/deep-learning-containers). \n",
    "\n",
    "The full list of HuggingFaceâ€™s pretrained BERT models can be found in the BERT section on this page https://huggingface.co/transformers/pretrained_models.html.\n",
    "\n",
    "This Jupyter Notebook was tested on a `ml.c5.4xlarge` SageMaker Notebook instance with `conda_pytorch_p310` kernel. You can set up your SageMaker Notebook instance by following the [Get Started with Amazon SageMaker Notebook Instances](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-console.html) documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5522ca52-6ca0-4087-8ed5-ecff50f91397",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install Dependencies:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca57475a-0ba3-49fe-b856-041ebb7e6e13",
   "metadata": {},
   "source": [
    "This tutorial requires the following pip packages:\n",
    "\n",
    "- torch-neuronx\n",
    "- neuronx-cc\n",
    "- transformers\n",
    "\n",
    "This notebook was tested with Neuron 2.13.2 and transformers 4.31.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed0160c-28c5-4815-a423-85b5d905eaee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600aa3d9-344e-414f-a0e1-6da2d4428955",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade sagemaker boto3 awscli "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b41b4d-dd8f-4918-82b7-16ec5988c0a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5502212-cd3f-468c-8a5c-87156b5f62da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade neuronx-cc==2.* torch-neuronx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1036c09-b21f-483a-80bf-c4887a18dda9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade transformers==4.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1993638-a797-4faa-8700-ebdaf64a4ada",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip list | grep -e neuron -e torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0bb14-8fc0-4076-aa5e-956ed71ec3a0",
   "metadata": {},
   "source": [
    "## Compile the model into an AWS Neuron optimized TorchScript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c004c4f-31cc-4c05-a5d7-6af861908616",
   "metadata": {},
   "source": [
    "In the following section, we load the BERT model and tokenizer, get a sample input, run inference on CPU, compile the model for Neuron using `torch_neuronx.trace()`, and save the optimized model as `TorchScript`.\n",
    "\n",
    "`torch_neuronx.trace()` expects a tensor or tuple of tensor inputs to use for tracing, so we unpack the tokenizer output using the `encode` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abcef04-8f53-4489-a1cf-e30c2e73ea19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab52cea2-88c0-41f2-b008-b767cdcaed48",
   "metadata": {},
   "source": [
    "You can ignore `ERROR  TDRV:tdrv_get_dev_info  No neuron device available` as this notebook is running on non-Inf2 instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4607d5bd-46ae-4dfc-a3b0-8b8535765f72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\", return_dict=False)\n",
    "\n",
    "# Setup some example inputs\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "max_length=128\n",
    "paraphrase = tokenizer.encode_plus(sequence_0, sequence_2, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Run the original PyTorch model on compilation exaple\n",
    "paraphrase_classification_logits = model(**paraphrase)[0]\n",
    "\n",
    "# Convert example inputs to a format that is compatible with TorchScript tracing\n",
    "example_inputs_paraphrase = paraphrase['input_ids'], paraphrase['attention_mask'], paraphrase['token_type_ids']\n",
    "example_inputs_not_paraphrase = not_paraphrase['input_ids'], not_paraphrase['attention_mask'], not_paraphrase['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8a246b-5007-4b88-bb33-e9339e21475b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run torch_neuronx.trace to generate a TorchScript that is optimized by AWS Neuron\n",
    "model_neuron = torch_neuronx.trace(model, example_inputs_paraphrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5dd0d2-4344-40f1-b797-ee5950516201",
   "metadata": {},
   "source": [
    "Save the compiled model, so it can be packaged and sent to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b7edc-0054-4aea-bd67-61be7947c7ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the TorchScript for later use\n",
    "model_neuron.save('neuron_compiled_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea97f538-aa6a-41ef-884a-7b01948b86a6",
   "metadata": {},
   "source": [
    "## Package the pre-trained model and upload it to S3\n",
    "\n",
    "To make the model available for the SageMaker deployment, you will TAR the serialized graph and upload it to the default Amazon S3 bucket for your SageMaker session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668d511f-f24a-4fcf-b6bc-dd019b520b10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now you'll create a model.tar.gz file to be used by SageMaker endpoint\n",
    "!tar -czvf model.tar.gz neuron_compiled_model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a34f8e6-2844-4f3e-978c-f91aa9609d51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, serializers, deserializers\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sess_bucket = sess.default_bucket()\n",
    "\n",
    "print(f'sagemaker role arn: {role}')\n",
    "print(f'sagemaker bucket: {sess_bucket}')\n",
    "print(f'sagemaker session region: {sess.boto_region_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4a9a61-89e1-438f-88ef-bfb12e7fc127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "prefix = \"inf2_compiled_model\"\n",
    "# create s3 uri\n",
    "s3_model_path = f\"s3://{sess_bucket}/{prefix}\"\n",
    "\n",
    "# upload model.tar.gz\n",
    "s3_model_uri = S3Uploader.upload(local_path=\"model.tar.gz\",desired_s3_uri=s3_model_path)\n",
    "print(f\"model artifcats uploaded to {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debd4d7f-038d-429e-9ab9-2eda17a6ee65",
   "metadata": {},
   "source": [
    "## Deploy Container and run inference based on the pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872bd679-0694-4b96-925a-75fe675ff3a4",
   "metadata": {},
   "source": [
    "To deploy a pretrained PyTorch model, you'll need to use the PyTorch estimator object to create a PyTorchModel object and set a different entry_point.\n",
    "\n",
    "You'll use the PyTorchModel object to deploy a PyTorchPredictor. This creates a SageMaker Endpoint -- a hosted prediction service that we can use to perform inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132a9fc5-1438-49dd-b1ed-cfcdbd62d3a0",
   "metadata": {},
   "source": [
    "An implementation of *model_fn* is required for inference script.\n",
    "We are going to implement our own **model_fn** and **predict_fn** for Hugging Face Bert, and use default implementations of **input_fn** and **output_fn** defined in sagemaker-pytorch-containers.\n",
    "\n",
    "In this example, the inference script is put in ***code*** folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedfd83f-257c-4e49-933f-dc55d3058fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"code\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e257dc6-6ce5-4cc7-b87f-e2bd807f9c48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "\n",
    "transformers==4.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b683e624-ac72-4dd6-9181-8b300969476c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "JSON_CONTENT_TYPE = 'application/json'\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    tokenizer_init = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "    model_file =os.path.join(model_dir, 'neuron_compiled_model.pt')\n",
    "    model_neuron = torch.jit.load(model_file)\n",
    "    return (model_neuron, tokenizer_init)\n",
    "\n",
    "\n",
    "def input_fn(serialized_input_data, content_type=JSON_CONTENT_TYPE):\n",
    "    if content_type == JSON_CONTENT_TYPE:\n",
    "        input_data = json.loads(serialized_input_data)\n",
    "        return input_data\n",
    "\n",
    "    else:\n",
    "        raise Exception('Requested unsupported ContentType in Accept: ' + content_type)\n",
    "        return\n",
    "\n",
    "\n",
    "def predict_fn(input_data, models):\n",
    "    model_bert, tokenizer = models\n",
    "    sequence_0 = input_data[0] \n",
    "    sequence_1 = input_data[1]\n",
    "    \n",
    "    max_length=128\n",
    "    paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    # Convert example inputs to a format that is compatible with TorchScript tracing\n",
    "    example_inputs_paraphrase = paraphrase['input_ids'], paraphrase['attention_mask'], paraphrase['token_type_ids']  \n",
    "\n",
    "    # Verify the TorchScript works on example inputs\n",
    "    paraphrase_classification_logits_neuron = model_bert(*example_inputs_paraphrase)\n",
    "    classes = ['not paraphrase', 'paraphrase']\n",
    "    paraphrase_prediction = paraphrase_classification_logits_neuron[0][0].argmax().item()\n",
    "    out_str = 'BERT says that \"{}\" and \"{}\" are {}'.format(sequence_0, sequence_1, classes[paraphrase_prediction])\n",
    "    \n",
    "    return out_str\n",
    "\n",
    "def output_fn(prediction_output, accept=JSON_CONTENT_TYPE):\n",
    "    if accept == JSON_CONTENT_TYPE:\n",
    "        return json.dumps(prediction_output), accept\n",
    "\n",
    "    raise Exception('Requested unsupported ContentType in Accept: ' + accept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1918127a-fcd4-4f45-9782-3f06a416fb75",
   "metadata": {},
   "source": [
    "Path of compiled pretrained model in S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46245367-83dc-4af0-8b2d-5f134f3ebe18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(s3_model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8a01d9-e7e5-4ba4-baf5-08f5ff184823",
   "metadata": {},
   "source": [
    "The model object is defined by using the SageMaker Python SDK's PyTorchModel and pass in the model from the estimator and the entry_point. The endpoint's entry point for inference is defined by model_fn as seen in the previous code block that prints out **inference.py**. The model_fn function will load the model and required tokenizer.\n",
    "\n",
    "Note, **image_uri** is the [Neuron Containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#neuron-containers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba75d76-bb74-4059-a058-c7f9c9a4720e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "ecr_image = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference-neuronx:2.1.2-neuronx-py310-sdk2.18.1-ubuntu20.04\"\n",
    "\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=s3_model_uri,\n",
    "    role=role,\n",
    "    source_dir=\"code\",\n",
    "    entry_point=\"inference.py\",\n",
    "    image_uri = ecr_image,\n",
    "    model_server_workers = 2\n",
    ")\n",
    "\n",
    "# Let SageMaker know that we've already compiled the model via neuron-cc\n",
    "pytorch_model._is_compiled_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96130d27-77bf-4a3d-bcf1-eddb5f0a2669",
   "metadata": {},
   "source": [
    "The arguments to the deploy function allow us to set the number and type of instances that will be used for the Endpoint.\n",
    "\n",
    "Here you will deploy the model to a single **ml.inf2.xlarge** instance.\n",
    "It may take 6-10 min to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e314c8-a99b-4f76-b5b6-5c854d5e6530",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictor = pytorch_model.deploy(\n",
    "    instance_type=\"ml.inf2.xlarge\",\n",
    "    initial_instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c8afcc-01e6-49b6-8040-dedd71257459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbeea2e-6ff1-4eee-8fc6-c7a779c81c20",
   "metadata": {},
   "source": [
    "Since in the input_fn we declared that the incoming requests are json-encoded, we need to use a json serializer, to encode the incoming data into a json string. Also, we declared the return content type to be json string, we Need to use a json deserializer to parse the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd1078-8bfc-41ec-970d-24c4915c0f00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fc23f6-ebd4-494b-88b1-39c38864f064",
   "metadata": {},
   "source": [
    "Using a list of sentences, now SageMaker endpoint is invoked to get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab08e45f-5767-49f7-892a-c7dde80a0dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result = predictor.predict(\n",
    "    [\n",
    "        \"Never allow the same bug to bite you twice.\",\n",
    "        \"The best part of Amazon SageMaker is that it makes machine learning easy.\",\n",
    "    ]\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3cfc75-30da-4592-a43b-af001daca358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result = predictor.predict(\n",
    "    [\n",
    "        \"The company HuggingFace is based in New York City\",\n",
    "        \"HuggingFace's headquarters are situated in Manhattan\",\n",
    "    ]\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a725e17f-014e-4b7d-b2dc-6c6e42bcf0a1",
   "metadata": {},
   "source": [
    "## Benchmarking your endpoint\n",
    "\n",
    "The following cells create a load test for your endpoint. You first define some helper functions: `inference_latency` runs the endpoint request, collects cliend side latency and any errors, `random_sentence` builds random to be sent to the endpoint.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b8eefb-ff41-4d28-a342-2098115c21f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "import boto3   \n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4822a5ca-49ca-4461-b1ce-f5429eaedd56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference_latency(model,*inputs):\n",
    "    \"\"\"\n",
    "    infetence_time is a simple method to return the latency of a model inference.\n",
    "\n",
    "        Parameters:\n",
    "            model: torch model onbject loaded using torch.jit.load\n",
    "            inputs: model() args\n",
    "\n",
    "        Returns:\n",
    "            latency in seconds\n",
    "    \"\"\"\n",
    "    error = False\n",
    "    start = time.time()\n",
    "    try:\n",
    "        results = model(*inputs)\n",
    "    except:\n",
    "        error = True\n",
    "        results = []\n",
    "    return {'latency':time.time() - start, 'error': error, 'result': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e628eb86-f6a3-485d-b418-e1716b000a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_sentence():\n",
    "    \n",
    "    s_nouns = [\"A dude\", \"My mom\", \"The king\", \"Some guy\", \"A cat with rabies\", \"A sloth\", \"Your homie\", \"This cool guy my gardener met yesterday\", \"Superman\"]\n",
    "    p_nouns = [\"These dudes\", \"Both of my moms\", \"All the kings of the world\", \"Some guys\", \"All of a cattery's cats\", \"The multitude of sloths living under your bed\", \"Your homies\", \"Like, these, like, all these people\", \"Supermen\"]\n",
    "    s_verbs = [\"eats\", \"kicks\", \"gives\", \"treats\", \"meets with\", \"creates\", \"hacks\", \"configures\", \"spies on\", \"meows on\", \"flees from\", \"tries to automate\", \"explodes\"]\n",
    "    p_verbs = [\"eat\", \"kick\", \"give\", \"treat\", \"meet with\", \"create\", \"hack\", \"configure\", \"spy on\", \"meow on\", \"flee from\", \"try to automate\", \"explode\"]\n",
    "    infinitives = [\"to make a pie.\", \"for no apparent reason.\", \"because the sky is green.\", \"for a disease.\", \"to be able to make toast explode.\", \"to know more about archeology.\"]\n",
    "    \n",
    "    return (random.choice(s_nouns) + ' ' + random.choice(s_verbs) + ' ' + random.choice(s_nouns).lower() or random.choice(p_nouns).lower() + ' ' + random.choice(infinitives))\n",
    "\n",
    "print([random_sentence(), random_sentence()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6aa582-e84e-4e39-8a2b-06f0d74b03e0",
   "metadata": {},
   "source": [
    "The following cell creates `number_of_clients` concurrent threads to run `number_of_runs` requests. Once completed, a `boto3` CloudWatch client will query for the server side latency metrics for comparison.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b52aea6-7c88-456a-ab0a-af6c397700e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining Auxiliary variables\n",
    "number_of_clients = 2\n",
    "number_of_runs = 10000\n",
    "t = tqdm(range(number_of_runs),position=0, leave=True)\n",
    "\n",
    "# Starting parallel clients\n",
    "cw_start = datetime.datetime.utcnow()\n",
    "\n",
    "results = Parallel(n_jobs=number_of_clients,prefer=\"threads\")(delayed(inference_latency)(predictor.predict,[random_sentence(), random_sentence()]) for mod in t)\n",
    "avg_throughput = t.total/t.format_dict['elapsed']\n",
    "\n",
    "cw_end = datetime.datetime.utcnow() \n",
    "\n",
    "# Computing metrics and print\n",
    "latencies = [res['latency'] for res in results]\n",
    "errors = [res['error'] for res in results]\n",
    "error_p = sum(errors)/len(errors) *100\n",
    "p50 = np.quantile(latencies[-10000:],0.50) * 1000\n",
    "p90 = np.quantile(latencies[-10000:],0.95) * 1000\n",
    "p95 = np.quantile(latencies[-10000:],0.99) * 1000\n",
    "\n",
    "print(f'Avg Throughput: :{avg_throughput:.1f}\\n')\n",
    "print(f'50th Percentile Latency:{p50:.1f} ms')\n",
    "print(f'90th Percentile Latency:{p90:.1f} ms')\n",
    "print(f'95th Percentile Latency:{p95:.1f} ms\\n')\n",
    "print(f'Errors percentage: {error_p:.1f} %\\n')\n",
    "\n",
    "# Querying CloudWatch\n",
    "print('Getting Cloudwatch:')\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "statistics=['SampleCount', 'Average', 'Minimum', 'Maximum']\n",
    "extended=['p50', 'p90', 'p95', 'p100']\n",
    "\n",
    "# Give 5 minute buffer to end\n",
    "cw_end += datetime.timedelta(minutes=5)\n",
    "\n",
    "# Period must be 1, 5, 10, 30, or multiple of 60\n",
    "# Calculate closest multiple of 60 to the total elapsed time\n",
    "factor = math.ceil((cw_end - cw_start).total_seconds() / 60)\n",
    "period = factor * 60\n",
    "print('Time elapsed: {} seconds'.format((cw_end - cw_start).total_seconds()))\n",
    "print('Using period of {} seconds\\n'.format(period))\n",
    "\n",
    "cloudwatch_ready = False\n",
    "# Keep polling CloudWatch metrics until datapoints are available\n",
    "while not cloudwatch_ready:\n",
    "  time.sleep(30)\n",
    "  print('Waiting 30 seconds ...')\n",
    "  # Must use default units of microseconds\n",
    "  model_latency_metrics = cloudwatch.get_metric_statistics(MetricName='ModelLatency',\n",
    "                                             Dimensions=[{'Name': 'EndpointName',\n",
    "                                                          'Value': predictor.endpoint_name},\n",
    "                                                         {'Name': 'VariantName',\n",
    "                                                          'Value': \"AllTraffic\"}],\n",
    "                                             Namespace=\"AWS/SageMaker\",\n",
    "                                             StartTime=cw_start,\n",
    "                                             EndTime=cw_end,\n",
    "                                             Period=period,\n",
    "                                             Statistics=statistics,\n",
    "                                             ExtendedStatistics=extended\n",
    "                                             )\n",
    "\n",
    "  if len(model_latency_metrics['Datapoints']) > 0:\n",
    "    print('{} latency datapoints ready'.format(model_latency_metrics['Datapoints'][0]['SampleCount']))\n",
    "    side_avg = model_latency_metrics['Datapoints'][0]['Average'] / 1000\n",
    "    side_p50 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p50'] / 1000\n",
    "    side_p90 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p90'] / 1000\n",
    "    side_p95 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p95'] / 1000\n",
    "    side_p100 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p100'] / 1000\n",
    "    \n",
    "    print(f'50th Percentile Latency:{side_p50:.1f} ms')\n",
    "    print(f'90th Percentile Latency:{side_p90:.1f} ms')\n",
    "    print(f'95th Percentile Latency:{side_p95:.1f} ms\\n')\n",
    "\n",
    "    cloudwatch_ready = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e867d9-b7ac-4121-adfd-e675879fa182",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Endpoints should be deleted when no longer in use, to avoid costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e779c-12c2-4851-9790-6575d1620785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "pytorch",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
